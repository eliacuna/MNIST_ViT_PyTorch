{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_ViT_Classifier_Pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wV68VBA-WjaL"
      },
      "source": [
        "<h1>PyTorch MNIST Classification Using a Transformer</h1>\n",
        "Welcome to my implementation of MNIST Classification using PyTorch. In this implementation I will be applying the Vision Transformer (Dosovitskiy et al., 2020) to MNIST Classification using a Google Colab notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p4pYFyPaXOeT",
        "outputId": "c7916ed2-c43e-4f48-efe7-e3e6a28b9328"
      },
      "source": [
        "!pip install vit-pytorch\n",
        "!pip install -U fvcore"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vit-pytorch in /usr/local/lib/python3.7/dist-packages (0.20.1)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from vit-pytorch) (0.10.0+cu102)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.7/dist-packages (from vit-pytorch) (0.3.0)\n",
            "Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.7/dist-packages (from vit-pytorch) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6->vit-pytorch) (3.7.4.3)\n",
            "Requirement already satisfied: pillow>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->vit-pytorch) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->vit-pytorch) (1.19.5)\n",
            "Collecting fvcore\n",
            "  Downloading fvcore-0.1.5.post20210804.tar.gz (49 kB)\n",
            "\u001b[K     |████████████████████████████████| 49 kB 3.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from fvcore) (1.19.5)\n",
            "Collecting yacs>=0.1.6\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 15.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from fvcore) (4.41.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.7/dist-packages (from fvcore) (1.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from fvcore) (7.1.2)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from fvcore) (0.8.9)\n",
            "Collecting iopath>=0.1.7\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Collecting portalocker\n",
            "  Downloading portalocker-2.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: fvcore\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20210804-py3-none-any.whl size=60618 sha256=d72dcf77f08e214457fb81e5d52c3510b18797be16b3164cc0efe1766c6f23e0\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/e2/fe/67887e71552be741faccead8f7a8e013b6e0b1225cf591afa1\n",
            "Successfully built fvcore\n",
            "Installing collected packages: pyyaml, portalocker, yacs, iopath, fvcore\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed fvcore-0.1.5.post20210804 iopath-0.1.9 portalocker-2.3.0 pyyaml-5.4.1 yacs-0.1.8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VH3m4z_nN6c"
      },
      "source": [
        "# Check which GPU Google has generously provided us :)\n",
        "!nvidia-smi -L"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svRP706QWefM"
      },
      "source": [
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "from torchvision.models import resnet50 as rn50\n",
        "from vit_pytorch import ViT\n",
        "from fvcore.nn import flop_count, flop_count_str, flop_count_table\n",
        "\n",
        "import time"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-pE8GAaOXq78"
      },
      "source": [
        "# Declare Vision Transformer model\n",
        "v = ViT(\n",
        "    image_size = 256,\n",
        "    patch_size = 32,\n",
        "    num_classes = 1000,\n",
        "    dim = 1024,\n",
        "    depth = 6,\n",
        "    heads = 16,\n",
        "    mlp_dim = 2048,\n",
        "    dropout = 0.1,\n",
        "    emb_dropout = 0.1\n",
        ")\n",
        "\n",
        "img = torch.randn(1, 3, 256, 256)\n",
        "\n",
        "preds = v(img) # (1, 1000)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BAowHUjIXxQq",
        "outputId": "170071a5-a5e7-426a-c411-fe73cf18114c"
      },
      "source": [
        "# Prepare the dataset\n",
        "\n",
        "n_epochs = 3\n",
        "batch_size_train = 64\n",
        "batch_size_test = 1000\n",
        "learning_rate = 0.01\n",
        "momentum = 0.5\n",
        "log_interval = 10\n",
        "\n",
        "random_seed = 1\n",
        "torch.backends.cudnn.enabled = False\n",
        "torch.manual_seed(random_seed)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb23afddbb0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBaf3dzIZO_7"
      },
      "source": [
        "Now lets load the training and test sets. This includes 60k images for training and 10k images for testing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxVXChetZL7j"
      },
      "source": [
        "# Load the dataset\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_train, shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
        "                             transform=torchvision.transforms.Compose([\n",
        "                               torchvision.transforms.ToTensor(),\n",
        "                               torchvision.transforms.Normalize(\n",
        "                                 (0.1307,), (0.3081,))\n",
        "                             ])),\n",
        "  batch_size=batch_size_test, shuffle=True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6suN3qUsaUmr"
      },
      "source": [
        "And now we will begin training on the training set that we just loaded. We will loop over the dataset 3 times and optimize learning on the fly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kFRg6pZgahsC"
      },
      "source": [
        "# Training the model\n",
        "\n",
        "def train_epoch(model, optimizer, data_loader, loss_history):\n",
        "    total_samples = len(data_loader.dataset)\n",
        "    model.train()\n",
        "\n",
        "    for i, (data, target) in enumerate(data_loader):\n",
        "        optimizer.zero_grad()\n",
        "        output = F.log_softmax(model(data), dim=1)\n",
        "        loss = F.nll_loss(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print('[' +  '{:5}'.format(i * len(data)) + '/' + '{:5}'.format(total_samples) +\n",
        "                  ' (' + '{:3.0f}'.format(100 * i / len(data_loader)) + '%)]  Loss: ' +\n",
        "                  '{:6.4f}'.format(loss.item()))\n",
        "            loss_history.append(loss.item())"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqKFW8sVauYa"
      },
      "source": [
        "# Evaluate the model on our test set\n",
        "\n",
        "def evaluate(model, data_loader, loss_history):\n",
        "    model.eval()\n",
        "    \n",
        "    total_samples = len(data_loader.dataset)\n",
        "    correct_samples = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for data, target in data_loader:\n",
        "            output = F.log_softmax(model(data), dim=1)\n",
        "            loss = F.nll_loss(output, target, reduction='sum')\n",
        "            _, pred = torch.max(output, dim=1)\n",
        "            \n",
        "            total_loss += loss.item()\n",
        "            correct_samples += pred.eq(target).sum()\n",
        "\n",
        "    avg_loss = total_loss / total_samples\n",
        "    loss_history.append(avg_loss)\n",
        "    print('\\nAverage test loss: ' + '{:.4f}'.format(avg_loss) +\n",
        "          '  Accuracy:' + '{:5}'.format(correct_samples) + '/' +\n",
        "          '{:5}'.format(total_samples) + ' (' +\n",
        "          '{:4.2f}'.format(100.0 * correct_samples / total_samples) + '%)\\n')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwKTpgWAbtDS",
        "outputId": "1d5956d0-2c26-4da3-d94f-52745dfb21c5"
      },
      "source": [
        "# Run training\n",
        "N_EPOCHS = 25\n",
        "\n",
        "start_time = time.time()\n",
        "model = ViT(image_size=28, patch_size=7, num_classes=10, channels=1,\n",
        "            dim=64, depth=6, heads=8, mlp_dim=128)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "\n",
        "train_loss_history, test_loss_history = [], []\n",
        "for epoch in range(1, N_EPOCHS + 1):\n",
        "    print('Epoch:', epoch)\n",
        "    train_epoch(model, optimizer, train_loader, train_loss_history)\n",
        "    evaluate(model, test_loader, test_loss_history)\n",
        "\n",
        "print('Execution time:', '{:5.2f}'.format(time.time() - start_time), 'seconds')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 1\n",
            "[    0/60000 (  0%)]  Loss: 2.3265\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.3600\n",
            "[12800/60000 ( 21%)]  Loss: 0.3747\n",
            "[19200/60000 ( 32%)]  Loss: 0.1786\n",
            "[25600/60000 ( 43%)]  Loss: 0.2901\n",
            "[32000/60000 ( 53%)]  Loss: 0.5387\n",
            "[38400/60000 ( 64%)]  Loss: 0.2848\n",
            "[44800/60000 ( 75%)]  Loss: 0.2700\n",
            "[51200/60000 ( 85%)]  Loss: 0.4385\n",
            "[57600/60000 ( 96%)]  Loss: 0.0358\n",
            "\n",
            "Average test loss: 0.1710  Accuracy: 9472/10000 (94.72%)\n",
            "\n",
            "Epoch: 2\n",
            "[    0/60000 (  0%)]  Loss: 0.2468\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.2551\n",
            "[12800/60000 ( 21%)]  Loss: 0.1465\n",
            "[19200/60000 ( 32%)]  Loss: 0.1338\n",
            "[25600/60000 ( 43%)]  Loss: 0.3321\n",
            "[32000/60000 ( 53%)]  Loss: 0.2151\n",
            "[38400/60000 ( 64%)]  Loss: 0.0349\n",
            "[44800/60000 ( 75%)]  Loss: 0.0746\n",
            "[51200/60000 ( 85%)]  Loss: 0.1203\n",
            "[57600/60000 ( 96%)]  Loss: 0.2814\n",
            "\n",
            "Average test loss: 0.1323  Accuracy: 9579/10000 (95.79%)\n",
            "\n",
            "Epoch: 3\n",
            "[    0/60000 (  0%)]  Loss: 0.1233\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0926\n",
            "[12800/60000 ( 21%)]  Loss: 0.2811\n",
            "[19200/60000 ( 32%)]  Loss: 0.1336\n",
            "[25600/60000 ( 43%)]  Loss: 0.1558\n",
            "[32000/60000 ( 53%)]  Loss: 0.0868\n",
            "[38400/60000 ( 64%)]  Loss: 0.1397\n",
            "[44800/60000 ( 75%)]  Loss: 0.1145\n",
            "[51200/60000 ( 85%)]  Loss: 0.1833\n",
            "[57600/60000 ( 96%)]  Loss: 0.3302\n",
            "\n",
            "Average test loss: 0.1536  Accuracy: 9542/10000 (95.42%)\n",
            "\n",
            "Epoch: 4\n",
            "[    0/60000 (  0%)]  Loss: 0.1701\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0372\n",
            "[12800/60000 ( 21%)]  Loss: 0.1661\n",
            "[19200/60000 ( 32%)]  Loss: 0.0400\n",
            "[25600/60000 ( 43%)]  Loss: 0.1574\n",
            "[32000/60000 ( 53%)]  Loss: 0.2675\n",
            "[38400/60000 ( 64%)]  Loss: 0.0980\n",
            "[44800/60000 ( 75%)]  Loss: 0.1428\n",
            "[51200/60000 ( 85%)]  Loss: 0.0684\n",
            "[57600/60000 ( 96%)]  Loss: 0.1752\n",
            "\n",
            "Average test loss: 0.1163  Accuracy: 9633/10000 (96.33%)\n",
            "\n",
            "Epoch: 5\n",
            "[    0/60000 (  0%)]  Loss: 0.0838\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.1185\n",
            "[12800/60000 ( 21%)]  Loss: 0.1263\n",
            "[19200/60000 ( 32%)]  Loss: 0.1564\n",
            "[25600/60000 ( 43%)]  Loss: 0.1260\n",
            "[32000/60000 ( 53%)]  Loss: 0.1525\n",
            "[38400/60000 ( 64%)]  Loss: 0.1837\n",
            "[44800/60000 ( 75%)]  Loss: 0.0151\n",
            "[51200/60000 ( 85%)]  Loss: 0.1456\n",
            "[57600/60000 ( 96%)]  Loss: 0.0389\n",
            "\n",
            "Average test loss: 0.1060  Accuracy: 9655/10000 (96.55%)\n",
            "\n",
            "Epoch: 6\n",
            "[    0/60000 (  0%)]  Loss: 0.1016\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0442\n",
            "[12800/60000 ( 21%)]  Loss: 0.0468\n",
            "[19200/60000 ( 32%)]  Loss: 0.2204\n",
            "[25600/60000 ( 43%)]  Loss: 0.0152\n",
            "[32000/60000 ( 53%)]  Loss: 0.0302\n",
            "[38400/60000 ( 64%)]  Loss: 0.0702\n",
            "[44800/60000 ( 75%)]  Loss: 0.0525\n",
            "[51200/60000 ( 85%)]  Loss: 0.0492\n",
            "[57600/60000 ( 96%)]  Loss: 0.1182\n",
            "\n",
            "Average test loss: 0.0855  Accuracy: 9732/10000 (97.32%)\n",
            "\n",
            "Epoch: 7\n",
            "[    0/60000 (  0%)]  Loss: 0.0154\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0981\n",
            "[12800/60000 ( 21%)]  Loss: 0.0926\n",
            "[19200/60000 ( 32%)]  Loss: 0.1245\n",
            "[25600/60000 ( 43%)]  Loss: 0.0529\n",
            "[32000/60000 ( 53%)]  Loss: 0.0401\n",
            "[38400/60000 ( 64%)]  Loss: 0.0245\n",
            "[44800/60000 ( 75%)]  Loss: 0.0399\n",
            "[51200/60000 ( 85%)]  Loss: 0.0143\n",
            "[57600/60000 ( 96%)]  Loss: 0.1125\n",
            "\n",
            "Average test loss: 0.0835  Accuracy: 9735/10000 (97.35%)\n",
            "\n",
            "Epoch: 8\n",
            "[    0/60000 (  0%)]  Loss: 0.0457\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0088\n",
            "[12800/60000 ( 21%)]  Loss: 0.0853\n",
            "[19200/60000 ( 32%)]  Loss: 0.1069\n",
            "[25600/60000 ( 43%)]  Loss: 0.0835\n",
            "[32000/60000 ( 53%)]  Loss: 0.1386\n",
            "[38400/60000 ( 64%)]  Loss: 0.1665\n",
            "[44800/60000 ( 75%)]  Loss: 0.0213\n",
            "[51200/60000 ( 85%)]  Loss: 0.1528\n",
            "[57600/60000 ( 96%)]  Loss: 0.0307\n",
            "\n",
            "Average test loss: 0.0899  Accuracy: 9730/10000 (97.30%)\n",
            "\n",
            "Epoch: 9\n",
            "[    0/60000 (  0%)]  Loss: 0.1164\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0784\n",
            "[12800/60000 ( 21%)]  Loss: 0.1068\n",
            "[19200/60000 ( 32%)]  Loss: 0.2309\n",
            "[25600/60000 ( 43%)]  Loss: 0.0110\n",
            "[32000/60000 ( 53%)]  Loss: 0.0801\n",
            "[38400/60000 ( 64%)]  Loss: 0.1190\n",
            "[44800/60000 ( 75%)]  Loss: 0.0446\n",
            "[51200/60000 ( 85%)]  Loss: 0.1906\n",
            "[57600/60000 ( 96%)]  Loss: 0.0358\n",
            "\n",
            "Average test loss: 0.0837  Accuracy: 9739/10000 (97.39%)\n",
            "\n",
            "Epoch: 10\n",
            "[    0/60000 (  0%)]  Loss: 0.0486\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0603\n",
            "[12800/60000 ( 21%)]  Loss: 0.0541\n",
            "[19200/60000 ( 32%)]  Loss: 0.0632\n",
            "[25600/60000 ( 43%)]  Loss: 0.0198\n",
            "[32000/60000 ( 53%)]  Loss: 0.0748\n",
            "[38400/60000 ( 64%)]  Loss: 0.0531\n",
            "[44800/60000 ( 75%)]  Loss: 0.0100\n",
            "[51200/60000 ( 85%)]  Loss: 0.1014\n",
            "[57600/60000 ( 96%)]  Loss: 0.0020\n",
            "\n",
            "Average test loss: 0.0727  Accuracy: 9777/10000 (97.77%)\n",
            "\n",
            "Epoch: 11\n",
            "[    0/60000 (  0%)]  Loss: 0.0031\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0284\n",
            "[12800/60000 ( 21%)]  Loss: 0.0249\n",
            "[19200/60000 ( 32%)]  Loss: 0.0079\n",
            "[25600/60000 ( 43%)]  Loss: 0.0415\n",
            "[32000/60000 ( 53%)]  Loss: 0.0501\n",
            "[38400/60000 ( 64%)]  Loss: 0.0060\n",
            "[44800/60000 ( 75%)]  Loss: 0.0732\n",
            "[51200/60000 ( 85%)]  Loss: 0.0042\n",
            "[57600/60000 ( 96%)]  Loss: 0.0179\n",
            "\n",
            "Average test loss: 0.0719  Accuracy: 9783/10000 (97.83%)\n",
            "\n",
            "Epoch: 12\n",
            "[    0/60000 (  0%)]  Loss: 0.0386\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0107\n",
            "[12800/60000 ( 21%)]  Loss: 0.0673\n",
            "[19200/60000 ( 32%)]  Loss: 0.0901\n",
            "[25600/60000 ( 43%)]  Loss: 0.1035\n",
            "[32000/60000 ( 53%)]  Loss: 0.0546\n",
            "[38400/60000 ( 64%)]  Loss: 0.0305\n",
            "[44800/60000 ( 75%)]  Loss: 0.0723\n",
            "[51200/60000 ( 85%)]  Loss: 0.0028\n",
            "[57600/60000 ( 96%)]  Loss: 0.0139\n",
            "\n",
            "Average test loss: 0.0743  Accuracy: 9789/10000 (97.89%)\n",
            "\n",
            "Epoch: 13\n",
            "[    0/60000 (  0%)]  Loss: 0.0587\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0356\n",
            "[12800/60000 ( 21%)]  Loss: 0.0014\n",
            "[19200/60000 ( 32%)]  Loss: 0.0075\n",
            "[25600/60000 ( 43%)]  Loss: 0.0041\n",
            "[32000/60000 ( 53%)]  Loss: 0.0039\n",
            "[38400/60000 ( 64%)]  Loss: 0.0161\n",
            "[44800/60000 ( 75%)]  Loss: 0.0269\n",
            "[51200/60000 ( 85%)]  Loss: 0.0137\n",
            "[57600/60000 ( 96%)]  Loss: 0.0471\n",
            "\n",
            "Average test loss: 0.0804  Accuracy: 9757/10000 (97.57%)\n",
            "\n",
            "Epoch: 14\n",
            "[    0/60000 (  0%)]  Loss: 0.0086\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.1104\n",
            "[12800/60000 ( 21%)]  Loss: 0.0044\n",
            "[19200/60000 ( 32%)]  Loss: 0.0711\n",
            "[25600/60000 ( 43%)]  Loss: 0.0053\n",
            "[32000/60000 ( 53%)]  Loss: 0.0065\n",
            "[38400/60000 ( 64%)]  Loss: 0.0108\n",
            "[44800/60000 ( 75%)]  Loss: 0.0201\n",
            "[51200/60000 ( 85%)]  Loss: 0.0087\n",
            "[57600/60000 ( 96%)]  Loss: 0.0374\n",
            "\n",
            "Average test loss: 0.0733  Accuracy: 9788/10000 (97.88%)\n",
            "\n",
            "Epoch: 15\n",
            "[    0/60000 (  0%)]  Loss: 0.0325\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0204\n",
            "[12800/60000 ( 21%)]  Loss: 0.0403\n",
            "[19200/60000 ( 32%)]  Loss: 0.0929\n",
            "[25600/60000 ( 43%)]  Loss: 0.0395\n",
            "[32000/60000 ( 53%)]  Loss: 0.0387\n",
            "[38400/60000 ( 64%)]  Loss: 0.0499\n",
            "[44800/60000 ( 75%)]  Loss: 0.0217\n",
            "[51200/60000 ( 85%)]  Loss: 0.0321\n",
            "[57600/60000 ( 96%)]  Loss: 0.0105\n",
            "\n",
            "Average test loss: 0.0717  Accuracy: 9789/10000 (97.89%)\n",
            "\n",
            "Epoch: 16\n",
            "[    0/60000 (  0%)]  Loss: 0.0026\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0064\n",
            "[12800/60000 ( 21%)]  Loss: 0.0190\n",
            "[19200/60000 ( 32%)]  Loss: 0.0860\n",
            "[25600/60000 ( 43%)]  Loss: 0.0647\n",
            "[32000/60000 ( 53%)]  Loss: 0.0070\n",
            "[38400/60000 ( 64%)]  Loss: 0.0319\n",
            "[44800/60000 ( 75%)]  Loss: 0.0362\n",
            "[51200/60000 ( 85%)]  Loss: 0.0016\n",
            "[57600/60000 ( 96%)]  Loss: 0.0528\n",
            "\n",
            "Average test loss: 0.0753  Accuracy: 9779/10000 (97.79%)\n",
            "\n",
            "Epoch: 17\n",
            "[    0/60000 (  0%)]  Loss: 0.0028\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0108\n",
            "[12800/60000 ( 21%)]  Loss: 0.0276\n",
            "[19200/60000 ( 32%)]  Loss: 0.0012\n",
            "[25600/60000 ( 43%)]  Loss: 0.0048\n",
            "[32000/60000 ( 53%)]  Loss: 0.0238\n",
            "[38400/60000 ( 64%)]  Loss: 0.0911\n",
            "[44800/60000 ( 75%)]  Loss: 0.0519\n",
            "[51200/60000 ( 85%)]  Loss: 0.0185\n",
            "[57600/60000 ( 96%)]  Loss: 0.0134\n",
            "\n",
            "Average test loss: 0.0791  Accuracy: 9780/10000 (97.80%)\n",
            "\n",
            "Epoch: 18\n",
            "[    0/60000 (  0%)]  Loss: 0.0297\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0024\n",
            "[12800/60000 ( 21%)]  Loss: 0.0065\n",
            "[19200/60000 ( 32%)]  Loss: 0.0016\n",
            "[25600/60000 ( 43%)]  Loss: 0.0186\n",
            "[32000/60000 ( 53%)]  Loss: 0.0536\n",
            "[38400/60000 ( 64%)]  Loss: 0.0755\n",
            "[44800/60000 ( 75%)]  Loss: 0.0685\n",
            "[51200/60000 ( 85%)]  Loss: 0.0035\n",
            "[57600/60000 ( 96%)]  Loss: 0.0751\n",
            "\n",
            "Average test loss: 0.0643  Accuracy: 9808/10000 (98.08%)\n",
            "\n",
            "Epoch: 19\n",
            "[    0/60000 (  0%)]  Loss: 0.0067\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0048\n",
            "[12800/60000 ( 21%)]  Loss: 0.0155\n",
            "[19200/60000 ( 32%)]  Loss: 0.0116\n",
            "[25600/60000 ( 43%)]  Loss: 0.0022\n",
            "[32000/60000 ( 53%)]  Loss: 0.0337\n",
            "[38400/60000 ( 64%)]  Loss: 0.0279\n",
            "[44800/60000 ( 75%)]  Loss: 0.0360\n",
            "[51200/60000 ( 85%)]  Loss: 0.0178\n",
            "[57600/60000 ( 96%)]  Loss: 0.0064\n",
            "\n",
            "Average test loss: 0.0614  Accuracy: 9820/10000 (98.20%)\n",
            "\n",
            "Epoch: 20\n",
            "[    0/60000 (  0%)]  Loss: 0.0041\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0089\n",
            "[12800/60000 ( 21%)]  Loss: 0.0030\n",
            "[19200/60000 ( 32%)]  Loss: 0.0701\n",
            "[25600/60000 ( 43%)]  Loss: 0.0238\n",
            "[32000/60000 ( 53%)]  Loss: 0.0133\n",
            "[38400/60000 ( 64%)]  Loss: 0.0542\n",
            "[44800/60000 ( 75%)]  Loss: 0.0102\n",
            "[51200/60000 ( 85%)]  Loss: 0.0706\n",
            "[57600/60000 ( 96%)]  Loss: 0.0861\n",
            "\n",
            "Average test loss: 0.0788  Accuracy: 9763/10000 (97.63%)\n",
            "\n",
            "Epoch: 21\n",
            "[    0/60000 (  0%)]  Loss: 0.0470\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0093\n",
            "[12800/60000 ( 21%)]  Loss: 0.0056\n",
            "[19200/60000 ( 32%)]  Loss: 0.0147\n",
            "[25600/60000 ( 43%)]  Loss: 0.0701\n",
            "[32000/60000 ( 53%)]  Loss: 0.0241\n",
            "[38400/60000 ( 64%)]  Loss: 0.0012\n",
            "[44800/60000 ( 75%)]  Loss: 0.0565\n",
            "[51200/60000 ( 85%)]  Loss: 0.0166\n",
            "[57600/60000 ( 96%)]  Loss: 0.0057\n",
            "\n",
            "Average test loss: 0.0874  Accuracy: 9744/10000 (97.44%)\n",
            "\n",
            "Epoch: 22\n",
            "[    0/60000 (  0%)]  Loss: 0.0557\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0506\n",
            "[12800/60000 ( 21%)]  Loss: 0.0167\n",
            "[19200/60000 ( 32%)]  Loss: 0.0619\n",
            "[25600/60000 ( 43%)]  Loss: 0.0127\n",
            "[32000/60000 ( 53%)]  Loss: 0.0097\n",
            "[38400/60000 ( 64%)]  Loss: 0.0267\n",
            "[44800/60000 ( 75%)]  Loss: 0.0474\n",
            "[51200/60000 ( 85%)]  Loss: 0.0204\n",
            "[57600/60000 ( 96%)]  Loss: 0.0344\n",
            "\n",
            "Average test loss: 0.0734  Accuracy: 9807/10000 (98.07%)\n",
            "\n",
            "Epoch: 23\n",
            "[    0/60000 (  0%)]  Loss: 0.0096\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0121\n",
            "[12800/60000 ( 21%)]  Loss: 0.0591\n",
            "[19200/60000 ( 32%)]  Loss: 0.0866\n",
            "[25600/60000 ( 43%)]  Loss: 0.0006\n",
            "[32000/60000 ( 53%)]  Loss: 0.0045\n",
            "[38400/60000 ( 64%)]  Loss: 0.0048\n",
            "[44800/60000 ( 75%)]  Loss: 0.0408\n",
            "[51200/60000 ( 85%)]  Loss: 0.0102\n",
            "[57600/60000 ( 96%)]  Loss: 0.0934\n",
            "\n",
            "Average test loss: 0.0700  Accuracy: 9798/10000 (97.98%)\n",
            "\n",
            "Epoch: 24\n",
            "[    0/60000 (  0%)]  Loss: 0.0107\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0070\n",
            "[12800/60000 ( 21%)]  Loss: 0.0100\n",
            "[19200/60000 ( 32%)]  Loss: 0.0692\n",
            "[25600/60000 ( 43%)]  Loss: 0.0076\n",
            "[32000/60000 ( 53%)]  Loss: 0.0310\n",
            "[38400/60000 ( 64%)]  Loss: 0.0040\n",
            "[44800/60000 ( 75%)]  Loss: 0.0831\n",
            "[51200/60000 ( 85%)]  Loss: 0.0246\n",
            "[57600/60000 ( 96%)]  Loss: 0.0143\n",
            "\n",
            "Average test loss: 0.0660  Accuracy: 9809/10000 (98.09%)\n",
            "\n",
            "Epoch: 25\n",
            "[    0/60000 (  0%)]  Loss: 0.0060\n",
            "[ 6400/60000 ( 11%)]  Loss: 0.0121\n",
            "[12800/60000 ( 21%)]  Loss: 0.0681\n",
            "[19200/60000 ( 32%)]  Loss: 0.0186\n",
            "[25600/60000 ( 43%)]  Loss: 0.0007\n",
            "[32000/60000 ( 53%)]  Loss: 0.0121\n",
            "[38400/60000 ( 64%)]  Loss: 0.0135\n",
            "[44800/60000 ( 75%)]  Loss: 0.0755\n",
            "[51200/60000 ( 85%)]  Loss: 0.0052\n",
            "[57600/60000 ( 96%)]  Loss: 0.0116\n",
            "\n",
            "Average test loss: 0.0592  Accuracy: 9823/10000 (98.23%)\n",
            "\n",
            "Execution time: 5517.67 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}